{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Wstęp\n",
    "\n",
    "Celem projektu było przygotowanie i przetestowanie modelu klasyfikującego proste rysunki należące do dziesięciu różnych klas.\n",
    "\n",
    "Wykorzystano framework **PyTorch** oraz architekturę **ResNet-18** z wagami wstępnie wytrenowanymi na dużym zbiorze obrazów.\n",
    "\n",
    "W ramach pracy przeprowadzono analizę i wstępne przetwarzanie danych, podział zbioru na części treningową, walidacyjną i testową, a następnie proces uczenia i ewaluacji modelu."
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "IMPORTY\n",
    "\"\"\"\n",
    "\n",
    "from torch import optim\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, f1_score)\n",
    "from src.dataset import explore\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import splitfolders\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import itertools"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 67\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ],
   "id": "4f9c617b8959d7c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d6eb99288954d24",
   "metadata": {},
   "source": [
    "# Eksploracja danych\n",
    "\n",
    "W tym kroku wykorzystano funkcję `explore(DATA_ROOT, RANDOM_SEED)` z modułu `src.dataset`, aby **obejrzeć strukturę i przykładowe obrazy** oraz sprawdzić rozkład klas. Zbiór zawiera 10 klas szkiców/symboli:\n",
    "\n",
    "`anchor, balloon, bicycle, envelope, paper_boat, peace_symbol, smiley, speech_bubble, spiral, thumb`.\n",
    "\n",
    "**Po co eksploracja?** Pozwala szybko zweryfikować, czy dane są poprawnie zorganizowane w katalogach, czy klasy są zbalansowane oraz jak wyglądają „trudne” przypadki (np. podobne kształty). Na tej podstawie można lepiej zaplanować preprocessing i strategię uczenia (np. czy potrzebne będą augmentacje)."
   ]
  },
  {
   "cell_type": "code",
   "id": "d594fb938c597539",
   "metadata": {},
   "source": [
    "DATA_ROOT = \"../data\"\n",
    "explore(DATA_ROOT, RANDOM_SEED)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b65b3fe7aa6ea0c9",
   "metadata": {},
   "source": [
    "# Wnioski z eksploracji\n",
    "\n",
    "Z analizy danych widać, że:\n",
    "- Klasy są zbalansowane.\n",
    "- Wszystkie obrazy mają taką samą rozdzielczość.\n",
    "\n",
    "Rysunki wykonane ręcznie lub jako pieczątka mają trochę inne tło, często jest ciemniejsze albo widać na nim kratki czy linie.\n",
    "Nie dotyczy to jednak tylko jednej klasy, więc nie powinno to negatywnie wpłynąć na uczenie modelu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63becc2393a5d0",
   "metadata": {},
   "source": [
    "# Przygotowanie zbiorów\n",
    "\n",
    "Dane podzielono automatycznie na trzy części przy pomocy `splitfolders.ratio`:\n",
    "- **train: 70%**\n",
    "- **val: 15%**\n",
    "- **test: 15%**\n",
    "\n",
    "Taki podział daje wystarczającą liczbę przykładów do nauki, jednocześnie pozostawiając osobny zbiór testowy do końcowej oceny. Wydzielenie walidacji (15%) umożliwia monitorowanie jakości w trakcie trenowania i dobór hiperparametrów bez „podglądania” testu."
   ]
  },
  {
   "cell_type": "code",
   "id": "ddcfc6890edb6a33",
   "metadata": {},
   "source": [
    "input_folder = DATA_ROOT\n",
    "output_folder = \"../data_split\"\n",
    "\n",
    "splitfolders.ratio(\n",
    "    input_folder,\n",
    "    output=output_folder,\n",
    "    seed=42,\n",
    "    ratio=(.7, .15, .15),\n",
    "    group_prefix=None,\n",
    "    move=False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ad9e580e010abe5",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Zastosowane przekształcenia (dla `train` i `val`):\n",
    "- `transforms.ToTensor()` – konwersja obrazu do tensora PyTorch.\n",
    "- `transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])` – normalizacja do zakresu około **[-1, 1]**.\n",
    "\n",
    "**Uzasadnienie:**\n",
    "- Normalizacja przyspiesza i stabilizuje uczenie sieci konwolucyjnych (warstwy widzą dane w podobnej skali).\n",
    "- Zachowano **identyczny preprocessing** dla `train` i `val/test`, aby miara jakości była porównywalna.\n",
    "- Nie użyto augmentacji w tej wersji (świadomie, jako **bazowej konfiguracji**), aby najpierw sprawdzić jak daleko dojdziemy bez dodatkowych modyfikacji danych. W razie potrzeby można dodać np. `RandomRotation`, `RandomHorizontalFlip`, lekkie zmiany jasności/kontrastu, co zwykle poprawia uogólnianie na rysunkach/szkicach."
   ]
  },
  {
   "cell_type": "code",
   "id": "4d88b7ae3eaf651a",
   "metadata": {},
   "source": [
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "}\n",
    "\n",
    "train_dir = \"../data_split/train\"\n",
    "valid_dir = \"../data_split/val\"\n",
    "test_dir = \"../data_split/test\"\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, data_transforms[\"train\"])\n",
    "valid_ds = datasets.ImageFolder(valid_dir, data_transforms[\"val\"])\n",
    "test_ds = datasets.ImageFolder(test_dir, data_transforms[\"val\"])\n",
    "\n",
    "print(\"Kontrolne sprawdzenie klas:\", train_ds.classes)\n",
    "\n",
    "BATCH = 4\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(valid_ds,   batch_size=BATCH, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "data_loaders = {\"train\": train_loader, \"val\": val_loader}\n",
    "dataset_sizes = {\"train\": len(train_loader), \"val\": len(val_loader)}\n",
    "\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images = images * 0.5 + 0.5\n",
    "\n",
    "grid = torchvision.utils.make_grid(images[:16], nrow=8, padding=2)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(grid[0], cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Podgląd obrazów po preprocessingu\")\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a5465aa08802f16",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Wybrano **ResNet-18** z wagami wstępnie wytrenowanymi `weights='ResNet18_Weights.DEFAULT'` (transfer learning). Strategia uczenia:\n",
    "- **Zamrożono** wszystkie warstwy poza ostatnią (`fc`) – uczymy jedynie klasyfikator.\n",
    "- Funkcja kosztu: **CrossEntropyLoss**.\n",
    "- Optymalizator: **SGD** z `lr=0.001` i `momentum=0.9`.\n",
    "- Urządzenie: **CUDA** jeśli dostępna, w przeciwnym razie CPU.\n",
    "\n",
    "**Dlaczego tak?**\n",
    "- ResNet‑18 jest lekki i szybki, a wstępnie wytrenowane cechy konwolucyjne dobrze przenoszą się na nowe zadania wizji komputerowej nawet przy **niewielkiej liczbie danych**.\n",
    "- Zamrożenie backbone’u ogranicza **przeuczenie** i znacząco skraca czas treningu.\n",
    "- Użyto optymalizatora SGD z momentum, bo jest dość prosty i stabilny. Działa przewidywalnie i często daje lepsze wyniki niż metody takie jak Adam czy RMSProp, które szybciej się uczą, ale czasem prowadzą do gorszego uogólnienia modelu."
   ]
  },
  {
   "cell_type": "code",
   "id": "99cb77e3fadfc519",
   "metadata": {},
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "model = model.to(DEVICE)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d199e61a49cf24c1",
   "metadata": {},
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in data_loaders[phase]:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        num_samples = len(data_loaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double().item() / num_samples\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "\n",
    "\n",
    "        print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "777c89c9523eb5ad",
   "metadata": {},
   "source": [
    "torch.save(model.state_dict(), f\"model.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be307d9475bed510",
   "metadata": {},
   "source": [
    "# Ewaluacja\n",
    "\n",
    "Ewaluację przeprowadzono na **zbiorze testowym** (niewykorzystywanym podczas trenowania). Obliczono:\n",
    "- **Accuracy** oraz **macro F1**,\n",
    "- pełny **classification report** z precyzją i czułością dla każdej klasy,\n",
    "- **macierz pomyłek** (Confusion Matrix) z wizualizacją.\n",
    "\n",
    "**Wyniki (uzyskane na własnych testach):**\n",
    "- **Accuracy: 0.9595**\n",
    "- **Macro F1: 0.9592**\n",
    "- Przykładowe klasy: `envelope`, `smiley`, `spiral` osiągnęły **F1 = 1.0**;   słabsze wyniki odnotowano m.in. dla `speech_bubble` (**F1 ≈ 0.86**) oraz `thumb` (**precision ≈ 0.80**, recall = 1.0) i `paper_boat` (recall ≈ 0.86).\n",
    "\n",
    "**Interpretacja:**\n",
    "- Wysokie makro‑F1 potwierdza, że model **dobrze rozróżnia** klasy.\n",
    "- Pomyłki skupiają się na wizualnie podobnych kształtach co widać na macierzy pomyłek, mogą wynikać z braku augmentacji."
   ]
  },
  {
   "cell_type": "code",
   "id": "2b8c797c39967f35",
   "metadata": {},
   "source": [
    "model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "model.fc = nn.Linear(model.fc.in_features, 1000)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "new_model = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "new_model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "new_model.fc.weight.data = model.fc.weight.data[0:2]\n",
    "new_model.fc.bias.data = model.fc.bias.data[0:2]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35245a526e189c24",
   "metadata": {},
   "source": [
    "all_logits, all_probs, all_preds, all_targets, all_paths = [], [], [], [], []\n",
    "classes = ['anchor', 'balloon', 'bicycle', 'envelope', 'paper_boat', 'peace_symbol', 'smiley', 'speech_bubble', 'spiral', 'thumb']\n",
    "class_names = test_ds.classes\n",
    "num_classes = len(classes)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, y in test_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.softmax(logits, dim=1)[:,1]\n",
    "        preds = logits.argmax(1).cpu()\n",
    "\n",
    "        all_logits.append(logits.cpu())\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(y)\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "\n",
    "logits = torch.cat(all_logits).numpy()\n",
    "preds  = torch.cat(all_preds).numpy()\n",
    "targets= torch.cat(all_targets).numpy()\n",
    "probs  = torch.cat(all_probs).numpy()\n",
    "paths  = np.array(test_ds.samples)[:,0]\n",
    "\n",
    "\n",
    "acc = accuracy_score(targets, preds)\n",
    "f1m = f1_score(targets, preds, average=\"macro\")\n",
    "print(f\"\\nAccuracy: {acc:.4f} | Macro F1: {f1m:.4f}\\n\")\n",
    "print(classification_report(targets, preds, target_names=class_names, digits=4))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(targets, preds, labels=list(range(num_classes)))\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(num_classes)\n",
    "plt.xticks(tick_marks, class_names, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, class_names)\n",
    "th = cm.max()/2\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, cm[i, j], horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > th else \"black\")\n",
    "plt.ylabel('Prawdziwa klasa'); plt.xlabel('Predykcja')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52cb1c7646e9ac9f",
   "metadata": {},
   "source": [
    "# Wnioski\n",
    "\n",
    "1. **Skuteczność:** Prosty baseline z ResNet‑18 + zamrożony backbone + uczenie tylko warstwy `fc` dał **~96% accuracy** i **~0.96 macro‑F1** na zbiorze testowym (74 obrazów).\n",
    "2. **Co zadziałało:**\n",
    "   - Transfer learning z gotowymi wagami.\n",
    "   - Podział danych (70/15/15) i spójny preprocessing.\n",
    "3. **Miejsca do poprawy:**\n",
    "   - Dodać **augmentacje** (rotacje, odbicia, delikatne szumy/kontrast) to powinno ograniczyć pomyłki w podobnych klasach.\n",
    "   - Zapisywać **najlepszy model** na walidacji (checkpoint) i ładować go do testów.\n",
    "   - W kolejnych eksperymentach można rozważyć zmniejszenie liczby epok (np. do 20) przy zastosowaniu early stopping, co skróciłoby czas treningu bez pogorszenia wyników.\n",
    "4. **Dlaczego wybrano takie rozwiązania:**\n",
    "   - Celem było uzyskanie **solidnej, szybkiej bazy** bez skomplikowanych zależności i dlatego zamrożono backbone i użyto SGD.\n",
    "   - Pozwala to łatwo rozszerzyć pipeline: włączyć augmentacje, odblokować część warstw, podstroić hiperparametry, gdy zajdzie potrzeba.\n",
    "\n",
    "**Podsumowanie:** Otrzymany pipeline jest **czysty, powtarzalny i skuteczny**. Zaproponowane usprawnienia powinny pomóc przekroczyć 96% oraz poprawić najtrudniejsze klasy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
